{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81672177-e4eb-40a4-bb6e-4110d796ea29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:46.888605Z",
     "iopub.status.busy": "2025-01-09T21:00:46.888509Z",
     "iopub.status.idle": "2025-01-09T21:00:49.165294Z",
     "shell.execute_reply": "2025-01-09T21:00:49.164864Z",
     "shell.execute_reply.started": "2025-01-09T21:00:46.888594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 18:00:47 WARN Utils: Your hostname, baptvit resolves to a loopback address: 127.0.1.1; using 192.168.2.129 instead (on interface wlp4s0)\n",
      "25/01/09 18:00:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/baptvit/.ivy2/cache\n",
      "The jars for the packages stored in: /home/baptvit/.ivy2/jars\n",
      "io.acryl#datahub-spark-lineage added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0db2df45-8d82-4023-98bd-a85a755ef520;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.acryl#datahub-spark-lineage;0.14.0 in central\n",
      ":: resolution report :: resolve 62ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.acryl#datahub-spark-lineage;0.14.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0db2df45-8d82-4023-98bd-a85a755ef520\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 18:00:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .master(\"local[*]\") \\\n",
    "          .appName(\"spark-datahub-example\") \\\n",
    "          .config(\"spark.jars.packages\",\"io.acryl:datahub-spark-lineage:0.14.0\") \\\n",
    "          .config(\"spark.extraListeners\",\"datahub.spark.DatahubSparkListener\") \\\n",
    "          .config(\"spark.datahub.rest.server\", \"http://localhost:8080\") \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c099aa94-434c-4d9d-89d2-19ea5a9eac4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:49.165881Z",
     "iopub.status.busy": "2025-01-09T21:00:49.165760Z",
     "iopub.status.idle": "2025-01-09T21:00:49.461329Z",
     "shell.execute_reply": "2025-01-09T21:00:49.460968Z",
     "shell.execute_reply.started": "2025-01-09T21:00:49.165867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.129:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-datahub-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x723dbf2bd0d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa85dfe-cd8d-41b1-a942-eb5d2c8b457c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:49.462475Z",
     "iopub.status.busy": "2025-01-09T21:00:49.462228Z",
     "iopub.status.idle": "2025-01-09T21:00:51.672560Z",
     "shell.execute_reply": "2025-01-09T21:00:51.672172Z",
     "shell.execute_reply.started": "2025-01-09T21:00:49.462460Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "from datahub.emitter.mce_builder import make_dataset_urn\n",
    "from datahub.metadata.schema_classes import DatasetSnapshotClass, MetadataChangeEventClass\n",
    "\n",
    "# Load a dataset\n",
    "df = spark.read.csv(\"industry.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Capture dataset metadata\n",
    "# dataset_urn = make_dataset_urn(\"csv\", \"industry.csv\", \"PROD\")\n",
    "\n",
    "# # Create a DatasetSnapshot\n",
    "# dataset_snapshot = DatasetSnapshotClass(\n",
    "#     urn=dataset_urn,\n",
    "#     aspects=[],  # Add relevant aspects like schema, ownership, etc.\n",
    "# )\n",
    "\n",
    "# # Create a MetadataChangeEvent\n",
    "# mce = MetadataChangeEventClass(proposedSnapshot=dataset_snapshot)\n",
    "\n",
    "# # Emit the metadata to DataHub\n",
    "# emitter.emit(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd894576-1eec-4112-80ff-dcc784fae5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.673137Z",
     "iopub.status.busy": "2025-01-09T21:00:51.672951Z",
     "iopub.status.idle": "2025-01-09T21:00:51.769415Z",
     "shell.execute_reply": "2025-01-09T21:00:51.769028Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.673120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Industry|\n",
      "+--------------------+\n",
      "|  Accounting/Finance|\n",
      "|Advertising/Publi...|\n",
      "|  Aerospace/Aviation|\n",
      "|Arts/Entertainmen...|\n",
      "|          Automotive|\n",
      "|    Banking/Mortgage|\n",
      "|Business Development|\n",
      "|Business Opportunity|\n",
      "|Clerical/Administ...|\n",
      "|Construction/Faci...|\n",
      "|      Consumer Goods|\n",
      "|    Customer Service|\n",
      "|  Education/Training|\n",
      "|    Energy/Utilities|\n",
      "|         Engineering|\n",
      "| Government/Military|\n",
      "|               Green|\n",
      "|          Healthcare|\n",
      "|  Hospitality/Travel|\n",
      "|     Human Resources|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76ed0ac-decf-4292-b08a-fff533d77c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.770130Z",
     "iopub.status.busy": "2025-01-09T21:00:51.769852Z",
     "iopub.status.idle": "2025-01-09T21:00:51.892658Z",
     "shell.execute_reply": "2025-01-09T21:00:51.892349Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.770113Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "# Initialize the DataHub REST emitter\n",
    "emitter = DatahubRestEmitter(gms_server=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c150f7-187e-41f5-8c74-2fa49f12a33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.893137Z",
     "iopub.status.busy": "2025-01-09T21:00:51.893005Z",
     "iopub.status.idle": "2025-01-09T21:00:51.901207Z",
     "shell.execute_reply": "2025-01-09T21:00:51.900830Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.893121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Capture dataset metadata\n",
    "dataset_urn = make_dataset_urn(\"csv\", \"industry.csv\", \"PROD\")\n",
    "\n",
    "# Create a DatasetSnapshot\n",
    "dataset_snapshot = DatasetSnapshotClass(\n",
    "    urn=dataset_urn,\n",
    "    aspects=[],  # Add relevant aspects like schema, ownership, etc.\n",
    ")\n",
    "\n",
    "# Create a MetadataChangeEvent\n",
    "mce = MetadataChangeEventClass(proposedSnapshot=dataset_snapshot)\n",
    "\n",
    "# Emit the metadata to DataHub\n",
    "emitter.emit(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f30335-ed4e-489c-8397-98824b0fa60e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.901685Z",
     "iopub.status.busy": "2025-01-09T21:00:51.901575Z",
     "iopub.status.idle": "2025-01-09T21:00:52.075637Z",
     "shell.execute_reply": "2025-01-09T21:00:52.075219Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.901671Z"
    }
   },
   "outputs": [
    {
     "ename": "AvroTypeException",
     "evalue": "(<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'aspects': [SchemaMetadataClass({'schemaName': 'industry_example_test', 'platform': 'csv', 'version': 0, 'created': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'lastModified': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'deleted': None, 'dataset': None, 'cluster': None, 'hash': '123', 'platformSchema': 'spark', 'fields': [SchemaFieldClass({'fieldPath': 'Industry', 'jsonPath': None, 'nullable': False, 'description': None, 'label': None, 'created': None, 'lastModified': None, 'type': 'StringType()', 'nativeDataType': 'StringType()', 'recursive': False, 'globalTags': None, 'glossaryTerms': None, 'isPartOfKey': False, 'isPartitioningKey': None, 'jsonProps': None})], 'primaryKeys': None, 'foreignKeysSpecs': None, 'foreignKeys': None})]}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAvroTypeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_snapshot\u001b[38;5;241m.\u001b[39maspects\u001b[38;5;241m.\u001b[39mappend(schema_metadata)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Emit the updated metadata\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43memitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMetadataChangeEventClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_snapshot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:245\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit\u001b[0;34m(self, item, callback, async_flag)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_mcp(item, async_flag\u001b[38;5;241m=\u001b[39masync_flag)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit_mce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:257\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit_mce\u001b[0;34m(self, mce)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit_mce\u001b[39m(\u001b[38;5;28mself\u001b[39m, mce: MetadataChangeEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gms_server\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/entities?action=ingest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 257\u001b[0m     raw_mce_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     mce_obj \u001b[38;5;241m=\u001b[39m pre_json_transform(raw_mce_obj)\n\u001b[1;32m    259\u001b[0m     snapshot_fqn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.linkedin.metadata.snapshot.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmce\u001b[38;5;241m.\u001b[39mproposedSnapshot\u001b[38;5;241m.\u001b[39mRECORD_SCHEMA\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/dict_wrapper.py:57\u001b[0m, in \u001b[0;36mDictWrapper.to_obj\u001b[0;34m(self, tuples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, tuples: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     56\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_json_converter(tuples\u001b[38;5;241m=\u001b[39mtuples)\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRECORD_SCHEMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/avrojson.py:200\u001b[0m, in \u001b[0;36mAvroJsonConverter.to_json_object\u001b[0;34m(self, data_obj, writers_schema)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writers_schema, schema\u001b[38;5;241m.\u001b[39mSchema)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(writers_schema, data_obj):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AvroTypeException(writers_schema, data_obj)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generic_to_json(data_obj, writers_schema)\n",
      "\u001b[0;31mAvroTypeException\u001b[0m: (<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'aspects': [SchemaMetadataClass({'schemaName': 'industry_example_test', 'platform': 'csv', 'version': 0, 'created': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'lastModified': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'deleted': None, 'dataset': None, 'cluster': None, 'hash': '123', 'platformSchema': 'spark', 'fields': [SchemaFieldClass({'fieldPath': 'Industry', 'jsonPath': None, 'nullable': False, 'description': None, 'label': None, 'created': None, 'lastModified': None, 'type': 'StringType()', 'nativeDataType': 'StringType()', 'recursive': False, 'globalTags': None, 'glossaryTerms': None, 'isPartOfKey': False, 'isPartitioningKey': None, 'jsonProps': None})], 'primaryKeys': None, 'foreignKeysSpecs': None, 'foreignKeys': None})]}))"
     ]
    }
   ],
   "source": [
    "from datahub.metadata.schema_classes import SchemaMetadataClass, SchemaFieldClass\n",
    "\n",
    "# Extract schema from the DataFrame\n",
    "schema_fields = [\n",
    "    SchemaFieldClass(fieldPath=field.name, type=str(field.dataType), nativeDataType=str(field.dataType))\n",
    "    for field in df.schema.fields\n",
    "]\n",
    "\n",
    "# Create schema metadata\n",
    "schema_metadata = SchemaMetadataClass(\n",
    "    schemaName=\"industry_example_test\",\n",
    "    platform=\"csv\",\n",
    "    version=0,\n",
    "    fields=schema_fields,\n",
    "    hash=\"123\",\n",
    "    platformSchema=\"spark\",\n",
    ")\n",
    "\n",
    "# Add schema metadata to the dataset snapshot\n",
    "dataset_snapshot.aspects.append(schema_metadata)\n",
    "\n",
    "# Emit the updated metadata\n",
    "emitter.emit(MetadataChangeEventClass(proposedSnapshot=dataset_snapshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d09af4-8f1d-4944-8892-c6b06c6f702e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:01:00.019648Z",
     "iopub.status.busy": "2025-01-09T21:01:00.019447Z",
     "iopub.status.idle": "2025-01-09T21:01:00.065423Z",
     "shell.execute_reply": "2025-01-09T21:01:00.064826Z",
     "shell.execute_reply.started": "2025-01-09T21:01:00.019637Z"
    }
   },
   "outputs": [
    {
     "ename": "AvroTypeException",
     "evalue": "(<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,path/to/transformed_dataset.csv,PROD)', 'aspects': [UpstreamClass({'auditStamp': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'created': None, 'dataset': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'type': 'TRANSFORMED', 'properties': None, 'query': None})]}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAvroTypeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m lineage_snapshot \u001b[38;5;241m=\u001b[39m DatasetSnapshotClass(\n\u001b[1;32m     15\u001b[0m     urn\u001b[38;5;241m=\u001b[39mtransformed_dataset_urn,\n\u001b[1;32m     16\u001b[0m     aspects\u001b[38;5;241m=\u001b[39m[upstream],\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Emit the lineage metadata\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43memitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMetadataChangeEventClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineage_snapshot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:245\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit\u001b[0;34m(self, item, callback, async_flag)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_mcp(item, async_flag\u001b[38;5;241m=\u001b[39masync_flag)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit_mce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:257\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit_mce\u001b[0;34m(self, mce)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit_mce\u001b[39m(\u001b[38;5;28mself\u001b[39m, mce: MetadataChangeEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gms_server\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/entities?action=ingest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 257\u001b[0m     raw_mce_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     mce_obj \u001b[38;5;241m=\u001b[39m pre_json_transform(raw_mce_obj)\n\u001b[1;32m    259\u001b[0m     snapshot_fqn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.linkedin.metadata.snapshot.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmce\u001b[38;5;241m.\u001b[39mproposedSnapshot\u001b[38;5;241m.\u001b[39mRECORD_SCHEMA\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/dict_wrapper.py:57\u001b[0m, in \u001b[0;36mDictWrapper.to_obj\u001b[0;34m(self, tuples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, tuples: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     56\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_json_converter(tuples\u001b[38;5;241m=\u001b[39mtuples)\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRECORD_SCHEMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/avrojson.py:200\u001b[0m, in \u001b[0;36mAvroJsonConverter.to_json_object\u001b[0;34m(self, data_obj, writers_schema)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writers_schema, schema\u001b[38;5;241m.\u001b[39mSchema)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(writers_schema, data_obj):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AvroTypeException(writers_schema, data_obj)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generic_to_json(data_obj, writers_schema)\n",
      "\u001b[0;31mAvroTypeException\u001b[0m: (<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,path/to/transformed_dataset.csv,PROD)', 'aspects': [UpstreamClass({'auditStamp': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'created': None, 'dataset': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'type': 'TRANSFORMED', 'properties': None, 'query': None})]}))"
     ]
    }
   ],
   "source": [
    "from datahub.metadata.schema_classes import UpstreamClass, DatasetLineageTypeClass\n",
    "\n",
    "# Example: Transform the dataset\n",
    "transformed_df = df.withColumn(\"new_column_Industry\", df[\"Industry\"] * 2)\n",
    "\n",
    "# Capture lineage information\n",
    "upstream = UpstreamClass(\n",
    "    dataset=dataset_urn,\n",
    "    type=DatasetLineageTypeClass.TRANSFORMED,\n",
    ")\n",
    "\n",
    "# Create a lineage snapshot for the transformed dataset\n",
    "transformed_dataset_urn = make_dataset_urn(\"csv\", \"path/to/transformed_dataset.csv\", \"PROD\")\n",
    "lineage_snapshot = DatasetSnapshotClass(\n",
    "    urn=transformed_dataset_urn,\n",
    "    aspects=[upstream],\n",
    ")\n",
    "\n",
    "# Emit the lineage metadata\n",
    "emitter.emit(MetadataChangeEventClass(proposedSnapshot=lineage_snapshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a45b7c6-c849-4a4e-b680-37a1e425a1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:05:05.391628Z",
     "iopub.status.busy": "2025-01-09T21:05:05.391388Z",
     "iopub.status.idle": "2025-01-09T21:05:05.494213Z",
     "shell.execute_reply": "2025-01-09T21:05:05.493700Z",
     "shell.execute_reply.started": "2025-01-09T21:05:05.391611Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"output_v2\") # or df.write.saveAsTable(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a0239-ff4d-4fcc-b7b6-e6610435c885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
