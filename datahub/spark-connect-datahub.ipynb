{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc6c7eb-f1cf-4519-b7c1-f99dbec9cae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:08:51.175188Z",
     "iopub.status.busy": "2025-01-09T21:08:51.174708Z",
     "iopub.status.idle": "2025-01-09T21:08:51.371650Z",
     "shell.execute_reply": "2025-01-09T21:08:51.371292Z",
     "shell.execute_reply.started": "2025-01-09T21:08:51.175164Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "# Point to your DataHub REST endpoint\n",
    "emitter = DatahubRestEmitter(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51883e24-a163-4632-a9b0-2cedd4bbfb62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:08:51.986831Z",
     "iopub.status.busy": "2025-01-09T21:08:51.986586Z",
     "iopub.status.idle": "2025-01-09T21:08:52.026321Z",
     "shell.execute_reply": "2025-01-09T21:08:52.025872Z",
     "shell.execute_reply.started": "2025-01-09T21:08:51.986816Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahub.metadata.schema_classes import DatasetSnapshotClass, DatasetPropertiesClass, MetadataChangeEventClass\n",
    "\n",
    "# Create a dataset URN\n",
    "dataset_urn = \"urn:li:dataset:(urn:li:dataPlatform:spark,example_dataset,PROD)\"\n",
    "\n",
    "# Define dataset properties\n",
    "snapshot = DatasetSnapshotClass(\n",
    "    urn=dataset_urn,\n",
    "    aspects=[\n",
    "        DatasetPropertiesClass(\n",
    "            description=\"This dataset is an example generated by PySpark\",\n",
    "            customProperties={\"key\": \"value\"}\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Wrap in a Metadata Change Event (MCE)\n",
    "mce = MetadataChangeEventClass(proposedSnapshot=snapshot)\n",
    "\n",
    "# Emit the MCE\n",
    "emitter.emit_mce(mce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740bde5c-bf6c-4dda-bac9-c52344d2530a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:08:52.682215Z",
     "iopub.status.busy": "2025-01-09T21:08:52.682000Z",
     "iopub.status.idle": "2025-01-09T21:08:52.713340Z",
     "shell.execute_reply": "2025-01-09T21:08:52.712758Z",
     "shell.execute_reply.started": "2025-01-09T21:08:52.682198Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahub.metadata.schema_classes import UpstreamLineageClass, UpstreamClass\n",
    "\n",
    "# Define input and output datasets\n",
    "input_urn = \"urn:li:dataset:(urn:li:dataPlatform:spark,input_dataset,PROD)\"\n",
    "output_urn = \"urn:li:dataset:(urn:li:dataPlatform:spark,output_dataset,PROD)\"\n",
    "\n",
    "# Define upstream lineage\n",
    "lineage = UpstreamLineageClass(\n",
    "    upstreams=[\n",
    "        UpstreamClass(\n",
    "            dataset=input_urn,\n",
    "            type=\"TRANSFORMED\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Add lineage to the output dataset\n",
    "snapshot = DatasetSnapshotClass(\n",
    "    urn=output_urn,\n",
    "    aspects=[lineage]\n",
    ")\n",
    "\n",
    "# Emit the lineage\n",
    "mce = MetadataChangeEventClass(proposedSnapshot=snapshot)\n",
    "emitter.emit_mce(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81672177-e4eb-40a4-bb6e-4110d796ea29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:08:55.179942Z",
     "iopub.status.busy": "2025-01-09T21:08:55.179667Z",
     "iopub.status.idle": "2025-01-09T21:08:57.470933Z",
     "shell.execute_reply": "2025-01-09T21:08:57.470406Z",
     "shell.execute_reply.started": "2025-01-09T21:08:55.179920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 18:08:55 WARN Utils: Your hostname, baptvit resolves to a loopback address: 127.0.1.1; using 192.168.2.129 instead (on interface wlp4s0)\n",
      "25/01/09 18:08:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/baptvit/.ivy2/cache\n",
      "The jars for the packages stored in: /home/baptvit/.ivy2/jars\n",
      "io.acryl#datahub-spark-lineage added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1753987f-c714-466c-b522-7cb6afefe29a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.acryl#datahub-spark-lineage;0.14.0 in central\n",
      ":: resolution report :: resolve 61ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tio.acryl#datahub-spark-lineage;0.14.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1753987f-c714-466c-b522-7cb6afefe29a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 18:08:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 18:08:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .master(\"local[*]\") \\\n",
    "          .appName(\"spark-datahub-example\") \\\n",
    "          .config(\"spark.jars.packages\",\"io.acryl:datahub-spark-lineage:0.14.0\") \\\n",
    "          .config(\"spark.extraListeners\",\"datahub.spark.DatahubSparkListener\") \\\n",
    "          .config(\"spark.datahub.rest.server\", \"http://localhost:8080\") \\\n",
    "          .enableHiveSupport() \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c099aa94-434c-4d9d-89d2-19ea5a9eac4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:09:01.348498Z",
     "iopub.status.busy": "2025-01-09T21:09:01.347742Z",
     "iopub.status.idle": "2025-01-09T21:09:01.663652Z",
     "shell.execute_reply": "2025-01-09T21:09:01.663190Z",
     "shell.execute_reply.started": "2025-01-09T21:09:01.348454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.129:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-datahub-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x715646066bd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daa85dfe-cd8d-41b1-a942-eb5d2c8b457c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:09:02.538641Z",
     "iopub.status.busy": "2025-01-09T21:09:02.538442Z",
     "iopub.status.idle": "2025-01-09T21:09:04.358281Z",
     "shell.execute_reply": "2025-01-09T21:09:04.357889Z",
     "shell.execute_reply.started": "2025-01-09T21:09:02.538629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    }
   ],
   "source": [
    "from datahub.emitter.mce_builder import make_dataset_urn\n",
    "from datahub.metadata.schema_classes import DatasetSnapshotClass, MetadataChangeEventClass\n",
    "\n",
    "# Load a dataset\n",
    "df = spark.read.csv(\"industry.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Capture dataset metadata\n",
    "# dataset_urn = make_dataset_urn(\"csv\", \"industry.csv\", \"PROD\")\n",
    "\n",
    "# # Create a DatasetSnapshot\n",
    "# dataset_snapshot = DatasetSnapshotClass(\n",
    "#     urn=dataset_urn,\n",
    "#     aspects=[],  # Add relevant aspects like schema, ownership, etc.\n",
    "# )\n",
    "\n",
    "# # Create a MetadataChangeEvent\n",
    "# mce = MetadataChangeEventClass(proposedSnapshot=dataset_snapshot)\n",
    "\n",
    "# # Emit the metadata to DataHub\n",
    "# emitter.emit(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd894576-1eec-4112-80ff-dcc784fae5f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.673137Z",
     "iopub.status.busy": "2025-01-09T21:00:51.672951Z",
     "iopub.status.idle": "2025-01-09T21:00:51.769415Z",
     "shell.execute_reply": "2025-01-09T21:00:51.769028Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.673120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            Industry|\n",
      "+--------------------+\n",
      "|  Accounting/Finance|\n",
      "|Advertising/Publi...|\n",
      "|  Aerospace/Aviation|\n",
      "|Arts/Entertainmen...|\n",
      "|          Automotive|\n",
      "|    Banking/Mortgage|\n",
      "|Business Development|\n",
      "|Business Opportunity|\n",
      "|Clerical/Administ...|\n",
      "|Construction/Faci...|\n",
      "|      Consumer Goods|\n",
      "|    Customer Service|\n",
      "|  Education/Training|\n",
      "|    Energy/Utilities|\n",
      "|         Engineering|\n",
      "| Government/Military|\n",
      "|               Green|\n",
      "|          Healthcare|\n",
      "|  Hospitality/Travel|\n",
      "|     Human Resources|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba253ed-ed56-4876-a453-0387fc16f299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:09:46.483775Z",
     "iopub.status.busy": "2025-01-09T21:09:46.483400Z",
     "iopub.status.idle": "2025-01-09T21:09:46.735705Z",
     "shell.execute_reply": "2025-01-09T21:09:46.735322Z",
     "shell.execute_reply.started": "2025-01-09T21:09:46.483753Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed = transformed_df = df.withColumn(\"new_column_Industry\", df[\"Industry\"] * 2)\n",
    "df_transformed.write.csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89eb6fa-7e21-41c5-b0f8-9ee12bb27b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:09:54.020168Z",
     "iopub.status.busy": "2025-01-09T21:09:54.019711Z",
     "iopub.status.idle": "2025-01-09T21:09:54.064666Z",
     "shell.execute_reply": "2025-01-09T21:09:54.064105Z",
     "shell.execute_reply.started": "2025-01-09T21:09:54.020147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Emit input and output datasets\n",
    "input_urn = \"urn:li:dataset:(urn:li:dataPlatform:file,input_csv,PROD)\"\n",
    "output_urn = \"urn:li:dataset:(urn:li:dataPlatform:file,output_csv,PROD)\"\n",
    "\n",
    "# Log lineage\n",
    "lineage = UpstreamLineageClass(\n",
    "    upstreams=[\n",
    "        UpstreamClass(\n",
    "            dataset=input_urn,\n",
    "            type=\"TRANSFORMED\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "snapshot = DatasetSnapshotClass(\n",
    "    urn=output_urn,\n",
    "    aspects=[lineage]\n",
    ")\n",
    "mce = MetadataChangeEventClass(proposedSnapshot=snapshot)\n",
    "emitter.emit_mce(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ec2299-d186-4869-82e8-21025017cec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76ed0ac-decf-4292-b08a-fff533d77c40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.770130Z",
     "iopub.status.busy": "2025-01-09T21:00:51.769852Z",
     "iopub.status.idle": "2025-01-09T21:00:51.892658Z",
     "shell.execute_reply": "2025-01-09T21:00:51.892349Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.770113Z"
    }
   },
   "outputs": [],
   "source": [
    "from datahub.emitter.rest_emitter import DatahubRestEmitter\n",
    "\n",
    "# Initialize the DataHub REST emitter\n",
    "emitter = DatahubRestEmitter(gms_server=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c150f7-187e-41f5-8c74-2fa49f12a33f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.893137Z",
     "iopub.status.busy": "2025-01-09T21:00:51.893005Z",
     "iopub.status.idle": "2025-01-09T21:00:51.901207Z",
     "shell.execute_reply": "2025-01-09T21:00:51.900830Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.893121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Capture dataset metadata\n",
    "dataset_urn = make_dataset_urn(\"csv\", \"industry.csv\", \"PROD\")\n",
    "\n",
    "# Create a DatasetSnapshot\n",
    "dataset_snapshot = DatasetSnapshotClass(\n",
    "    urn=dataset_urn,\n",
    "    aspects=[],  # Add relevant aspects like schema, ownership, etc.\n",
    ")\n",
    "\n",
    "# Create a MetadataChangeEvent\n",
    "mce = MetadataChangeEventClass(proposedSnapshot=dataset_snapshot)\n",
    "\n",
    "# Emit the metadata to DataHub\n",
    "emitter.emit(mce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f30335-ed4e-489c-8397-98824b0fa60e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:00:51.901685Z",
     "iopub.status.busy": "2025-01-09T21:00:51.901575Z",
     "iopub.status.idle": "2025-01-09T21:00:52.075637Z",
     "shell.execute_reply": "2025-01-09T21:00:52.075219Z",
     "shell.execute_reply.started": "2025-01-09T21:00:51.901671Z"
    }
   },
   "outputs": [
    {
     "ename": "AvroTypeException",
     "evalue": "(<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'aspects': [SchemaMetadataClass({'schemaName': 'industry_example_test', 'platform': 'csv', 'version': 0, 'created': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'lastModified': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'deleted': None, 'dataset': None, 'cluster': None, 'hash': '123', 'platformSchema': 'spark', 'fields': [SchemaFieldClass({'fieldPath': 'Industry', 'jsonPath': None, 'nullable': False, 'description': None, 'label': None, 'created': None, 'lastModified': None, 'type': 'StringType()', 'nativeDataType': 'StringType()', 'recursive': False, 'globalTags': None, 'glossaryTerms': None, 'isPartOfKey': False, 'isPartitioningKey': None, 'jsonProps': None})], 'primaryKeys': None, 'foreignKeysSpecs': None, 'foreignKeys': None})]}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAvroTypeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset_snapshot\u001b[38;5;241m.\u001b[39maspects\u001b[38;5;241m.\u001b[39mappend(schema_metadata)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Emit the updated metadata\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43memitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMetadataChangeEventClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_snapshot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:245\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit\u001b[0;34m(self, item, callback, async_flag)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_mcp(item, async_flag\u001b[38;5;241m=\u001b[39masync_flag)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit_mce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:257\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit_mce\u001b[0;34m(self, mce)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit_mce\u001b[39m(\u001b[38;5;28mself\u001b[39m, mce: MetadataChangeEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gms_server\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/entities?action=ingest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 257\u001b[0m     raw_mce_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     mce_obj \u001b[38;5;241m=\u001b[39m pre_json_transform(raw_mce_obj)\n\u001b[1;32m    259\u001b[0m     snapshot_fqn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.linkedin.metadata.snapshot.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmce\u001b[38;5;241m.\u001b[39mproposedSnapshot\u001b[38;5;241m.\u001b[39mRECORD_SCHEMA\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/dict_wrapper.py:57\u001b[0m, in \u001b[0;36mDictWrapper.to_obj\u001b[0;34m(self, tuples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, tuples: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     56\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_json_converter(tuples\u001b[38;5;241m=\u001b[39mtuples)\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRECORD_SCHEMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/avrojson.py:200\u001b[0m, in \u001b[0;36mAvroJsonConverter.to_json_object\u001b[0;34m(self, data_obj, writers_schema)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writers_schema, schema\u001b[38;5;241m.\u001b[39mSchema)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(writers_schema, data_obj):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AvroTypeException(writers_schema, data_obj)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generic_to_json(data_obj, writers_schema)\n",
      "\u001b[0;31mAvroTypeException\u001b[0m: (<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'aspects': [SchemaMetadataClass({'schemaName': 'industry_example_test', 'platform': 'csv', 'version': 0, 'created': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'lastModified': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'deleted': None, 'dataset': None, 'cluster': None, 'hash': '123', 'platformSchema': 'spark', 'fields': [SchemaFieldClass({'fieldPath': 'Industry', 'jsonPath': None, 'nullable': False, 'description': None, 'label': None, 'created': None, 'lastModified': None, 'type': 'StringType()', 'nativeDataType': 'StringType()', 'recursive': False, 'globalTags': None, 'glossaryTerms': None, 'isPartOfKey': False, 'isPartitioningKey': None, 'jsonProps': None})], 'primaryKeys': None, 'foreignKeysSpecs': None, 'foreignKeys': None})]}))"
     ]
    }
   ],
   "source": [
    "from datahub.metadata.schema_classes import SchemaMetadataClass, SchemaFieldClass\n",
    "\n",
    "# Extract schema from the DataFrame\n",
    "schema_fields = [\n",
    "    SchemaFieldClass(fieldPath=field.name, type=str(field.dataType), nativeDataType=str(field.dataType))\n",
    "    for field in df.schema.fields\n",
    "]\n",
    "\n",
    "# Create schema metadata\n",
    "schema_metadata = SchemaMetadataClass(\n",
    "    schemaName=\"industry_example_test\",\n",
    "    platform=\"csv\",\n",
    "    version=0,\n",
    "    fields=schema_fields,\n",
    "    hash=\"123\",\n",
    "    platformSchema=\"spark\",\n",
    ")\n",
    "\n",
    "# Add schema metadata to the dataset snapshot\n",
    "dataset_snapshot.aspects.append(schema_metadata)\n",
    "\n",
    "# Emit the updated metadata\n",
    "emitter.emit(MetadataChangeEventClass(proposedSnapshot=dataset_snapshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d09af4-8f1d-4944-8892-c6b06c6f702e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:01:00.019648Z",
     "iopub.status.busy": "2025-01-09T21:01:00.019447Z",
     "iopub.status.idle": "2025-01-09T21:01:00.065423Z",
     "shell.execute_reply": "2025-01-09T21:01:00.064826Z",
     "shell.execute_reply.started": "2025-01-09T21:01:00.019637Z"
    }
   },
   "outputs": [
    {
     "ename": "AvroTypeException",
     "evalue": "(<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,path/to/transformed_dataset.csv,PROD)', 'aspects': [UpstreamClass({'auditStamp': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'created': None, 'dataset': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'type': 'TRANSFORMED', 'properties': None, 'query': None})]}))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAvroTypeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m lineage_snapshot \u001b[38;5;241m=\u001b[39m DatasetSnapshotClass(\n\u001b[1;32m     15\u001b[0m     urn\u001b[38;5;241m=\u001b[39mtransformed_dataset_urn,\n\u001b[1;32m     16\u001b[0m     aspects\u001b[38;5;241m=\u001b[39m[upstream],\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Emit the lineage metadata\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43memitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMetadataChangeEventClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineage_snapshot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:245\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit\u001b[0;34m(self, item, callback, async_flag)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memit_mcp(item, async_flag\u001b[38;5;241m=\u001b[39masync_flag)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit_mce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/emitter/rest_emitter.py:257\u001b[0m, in \u001b[0;36mDataHubRestEmitter.emit_mce\u001b[0;34m(self, mce)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memit_mce\u001b[39m(\u001b[38;5;28mself\u001b[39m, mce: MetadataChangeEvent) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gms_server\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/entities?action=ingest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 257\u001b[0m     raw_mce_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposedSnapshot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     mce_obj \u001b[38;5;241m=\u001b[39m pre_json_transform(raw_mce_obj)\n\u001b[1;32m    259\u001b[0m     snapshot_fqn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.linkedin.metadata.snapshot.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmce\u001b[38;5;241m.\u001b[39mproposedSnapshot\u001b[38;5;241m.\u001b[39mRECORD_SCHEMA\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/dict_wrapper.py:57\u001b[0m, in \u001b[0;36mDictWrapper.to_obj\u001b[0;34m(self, tuples)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, tuples: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     56\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_json_converter(tuples\u001b[38;5;241m=\u001b[39mtuples)\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_object\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRECORD_SCHEMA\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/avrogen/avrojson.py:200\u001b[0m, in \u001b[0;36mAvroJsonConverter.to_json_object\u001b[0;34m(self, data_obj, writers_schema)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(writers_schema, schema\u001b[38;5;241m.\u001b[39mSchema)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(writers_schema, data_obj):\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AvroTypeException(writers_schema, data_obj)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generic_to_json(data_obj, writers_schema)\n",
      "\u001b[0;31mAvroTypeException\u001b[0m: (<avro.schema.RecordSchema object at 0x723daf589ed0>, DatasetSnapshotClass({'urn': 'urn:li:dataset:(urn:li:dataPlatform:csv,path/to/transformed_dataset.csv,PROD)', 'aspects': [UpstreamClass({'auditStamp': AuditStampClass({'time': 0, 'actor': 'urn:li:corpuser:unknown', 'impersonator': None, 'message': None}), 'created': None, 'dataset': 'urn:li:dataset:(urn:li:dataPlatform:csv,industry.csv,PROD)', 'type': 'TRANSFORMED', 'properties': None, 'query': None})]}))"
     ]
    }
   ],
   "source": [
    "from datahub.metadata.schema_classes import UpstreamClass, DatasetLineageTypeClass\n",
    "\n",
    "# Example: Transform the dataset\n",
    "transformed_df = df.withColumn(\"new_column_Industry\", df[\"Industry\"] * 2)\n",
    "\n",
    "# Capture lineage information\n",
    "upstream = UpstreamClass(\n",
    "    dataset=dataset_urn,\n",
    "    type=DatasetLineageTypeClass.TRANSFORMED,\n",
    ")\n",
    "\n",
    "# Create a lineage snapshot for the transformed dataset\n",
    "transformed_dataset_urn = make_dataset_urn(\"csv\", \"path/to/transformed_dataset.csv\", \"PROD\")\n",
    "lineage_snapshot = DatasetSnapshotClass(\n",
    "    urn=transformed_dataset_urn,\n",
    "    aspects=[upstream],\n",
    ")\n",
    "\n",
    "# Emit the lineage metadata\n",
    "emitter.emit(MetadataChangeEventClass(proposedSnapshot=lineage_snapshot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a45b7c6-c849-4a4e-b680-37a1e425a1c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:05:05.391628Z",
     "iopub.status.busy": "2025-01-09T21:05:05.391388Z",
     "iopub.status.idle": "2025-01-09T21:05:05.494213Z",
     "shell.execute_reply": "2025-01-09T21:05:05.493700Z",
     "shell.execute_reply.started": "2025-01-09T21:05:05.391611Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet(\"output_v2\") # or df.write.saveAsTable(\"my_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "508e5f53-9c3e-486f-9ccd-8d5f4cb2c681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-09T21:04:01.241011Z",
     "iopub.status.busy": "2025-01-09T21:04:01.240328Z",
     "iopub.status.idle": "2025-01-09T21:04:01.255302Z",
     "shell.execute_reply": "2025-01-09T21:04:01.254916Z",
     "shell.execute_reply.started": "2025-01-09T21:04:01.240978Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataFlowClass' from 'datahub.metadata.schema_classes' (/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/metadata/schema_classes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatahub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetPropertiesClass, OtherSchemaClass, SchemaMetadataClass, SchemaFieldClass, DataPlatformInstanceClass\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatahub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataProcessInstancePropertiesClass, DataProcessInstanceRunEventClass, StatusClass\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatahub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema_classes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFlowClass, DataJobClass\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Perform some Spark operations\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/output_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# or df.write.saveAsTable(\"my_table\")\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataFlowClass' from 'datahub.metadata.schema_classes' (/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/datahub/metadata/schema_classes.py)"
     ]
    }
   ],
   "source": [
    "from datahub.emitter.mcp import MetadataChangeProposalWrapper\n",
    "from datahub.metadata.schema_classes import DatasetPropertiesClass, OtherSchemaClass, SchemaMetadataClass, SchemaFieldClass, DataPlatformInstanceClass\n",
    "from datahub.metadata.schema_classes import DataProcessInstancePropertiesClass, DataProcessInstanceRunEventClass, StatusClass\n",
    "from datahub.metadata.schema_classes import DataFlowClass, DataJobClass\n",
    "\n",
    "# Perform some Spark operations\n",
    "df.write.parquet(\"/tmp/output_v2\") # or df.write.saveAsTable(\"my_table\")\n",
    "\n",
    "# Emit dataflow and datajob manually for more control\n",
    "dataflow_urn = \"urn:li:dataFlow:(spark,my_spark_dataflow,prod)\"\n",
    "datajob_urn = \"urn:li:dataJob:(spark,my_spark_dataflow,my_spark_job)\"\n",
    "\n",
    "dataflow_mcp = MetadataChangeProposalWrapper(\n",
    "    entityUrn=dataflow_urn,\n",
    "    aspect=DataFlowClass(name=\"my_spark_dataflow\", platform=\"spark\"),\n",
    "    changeCategory=ChangeCategory.CREATE,\n",
    ")\n",
    "emitter.emit_mcp(dataflow_mcp)\n",
    "\n",
    "datajob_mcp = MetadataChangeProposalWrapper(\n",
    "    entityUrn=datajob_urn,\n",
    "    aspect=DataJobClass(name=\"my_spark_job\", flow=dataflow_urn, platform=\"spark\"),\n",
    "    changeCategory=ChangeCategory.CREATE,\n",
    ")\n",
    "emitter.emit_mcp(datajob_mcp)\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "\n",
    "# Flush the emitter to ensure all metadata is sent\n",
    "emitter.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a0239-ff4d-4fcc-b7b6-e6610435c885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
