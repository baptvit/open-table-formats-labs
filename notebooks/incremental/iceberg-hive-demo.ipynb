{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b62605-913b-41a6-8920-4743e6865aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:27.032049Z",
     "iopub.status.busy": "2025-01-10T20:31:27.031752Z",
     "iopub.status.idle": "2025-01-10T20:31:27.065971Z",
     "shell.execute_reply": "2025-01-10T20:31:27.065613Z",
     "shell.execute_reply.started": "2025-01-10T20:31:27.032031Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d07b1d-2be6-4b5d-bbbb-00270bba8367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:27.066452Z",
     "iopub.status.busy": "2025-01-10T20:31:27.066353Z",
     "iopub.status.idle": "2025-01-10T20:31:27.095175Z",
     "shell.execute_reply": "2025-01-10T20:31:27.094869Z",
     "shell.execute_reply.started": "2025-01-10T20:31:27.066442Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40940756-17f4-44f7-ade4-8cc6a448538a",
   "metadata": {},
   "source": [
    "### Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d262d6-90c2-40f7-b4db-5ed3ec455956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:27.095872Z",
     "iopub.status.busy": "2025-01-10T20:31:27.095769Z",
     "iopub.status.idle": "2025-01-10T20:31:27.097672Z",
     "shell.execute_reply": "2025-01-10T20:31:27.097380Z",
     "shell.execute_reply.started": "2025-01-10T20:31:27.095861Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_jar_packages = \",\".join([\n",
    "    \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\",\n",
    "    # \"org.apache.hive:hive-metastore:3.1.3\",\n",
    "    # \"org.apache.hive:hive-exec:3.1.3\",\n",
    "    # \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    # \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d417510-85ba-4981-a1c1-4b949381e70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:27.098028Z",
     "iopub.status.busy": "2025-01-10T20:31:27.097932Z",
     "iopub.status.idle": "2025-01-10T20:31:29.171107Z",
     "shell.execute_reply": "2025-01-10T20:31:29.170661Z",
     "shell.execute_reply.started": "2025-01-10T20:31:27.098019Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 17:31:27 WARN Utils: Your hostname, baptvit resolves to a loopback address: 127.0.1.1; using 192.168.2.129 instead (on interface wlp4s0)\n",
      "25/01/10 17:31:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/baptvit/.ivy2/cache\n",
      "The jars for the packages stored in: /home/baptvit/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3908ac0b-8404-4142-accd-954a938feef0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 62ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3908ac0b-8404-4142-accd-954a938feef0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n",
      "25/01/10 17:31:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43600)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/baptvit/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"iceberg-hive-playground\")\n",
    "    .config(\"spark.jars.packages\", spark_jar_packages)\n",
    "\n",
    "    # Icerbeg-Hive Integration\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "\n",
    "    # # S3 (MinIO Integration)\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-east-1\")\n",
    "\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9789e3b-bdeb-42f9-9910-6a5a1d939ee6",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa07f69-02fe-4deb-a14c-59de36b3c08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:29.172262Z",
     "iopub.status.busy": "2025-01-10T20:31:29.171984Z",
     "iopub.status.idle": "2025-01-10T20:31:29.175392Z",
     "shell.execute_reply": "2025-01-10T20:31:29.175056Z",
     "shell.execute_reply.started": "2025-01-10T20:31:29.172241Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_entry(faker: Faker, country_codes: list):\n",
    "    return {\n",
    "        \"id\": faker.unique.uuid4(),\n",
    "        \"name\":  faker.name(),\n",
    "        \"email\": faker.email(),\n",
    "        \"passport\": faker.passport_number(),\n",
    "        \"country_code\": random.choice(country_codes),\n",
    "        \"iban\": faker.iban(),\n",
    "        \"swift\": faker.swift11(),\n",
    "        \"created_at\": faker.past_date(start_date='-90d').strftime('%Y-%m-%d')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c245a4-9ace-4bf1-9897-4a8d186ac5f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:29.175980Z",
     "iopub.status.busy": "2025-01-10T20:31:29.175763Z",
     "iopub.status.idle": "2025-01-10T20:31:29.184851Z",
     "shell.execute_reply": "2025-01-10T20:31:29.184536Z",
     "shell.execute_reply.started": "2025-01-10T20:31:29.175966Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dataset(num: int, seed: int):\n",
    "    country_codes = ['US', 'CA', 'JP', 'KR', 'FR', 'GE', 'UK', 'BR', 'AR']\n",
    "    Faker.seed(seed)\n",
    "    faker = Faker()\n",
    "    return [generate_entry(faker, country_codes) for _ in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599e5cd7-d544-4b4e-9b89-09b483fef953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:29.289065Z",
     "iopub.status.busy": "2025-01-10T20:31:29.288695Z",
     "iopub.status.idle": "2025-01-10T20:31:29.313965Z",
     "shell.execute_reply": "2025-01-10T20:31:29.313581Z",
     "shell.execute_reply.started": "2025-01-10T20:31:29.289048Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = generate_dataset(num=5, seed=739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e985a4-3ccc-4b2b-a30f-ee40107bc10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:29.513892Z",
     "iopub.status.busy": "2025-01-10T20:31:29.513398Z",
     "iopub.status.idle": "2025-01-10T20:31:30.474993Z",
     "shell.execute_reply": "2025-01-10T20:31:30.474580Z",
     "shell.execute_reply.started": "2025-01-10T20:31:29.513867Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(dataset)\\\n",
    "        .withColumn(\"year\", year(col(\"created_at\")))\\\n",
    "        .withColumn(\"month\", month(col(\"created_at\")))\\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"created_at\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb1ff2-6a95-4e4f-8cce-443d04779f9f",
   "metadata": {},
   "source": [
    "### Iceberg-Hive Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe05cab-a79a-4114-be6d-dd5748c4de8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:30.685229Z",
     "iopub.status.busy": "2025-01-10T20:31:30.684761Z",
     "iopub.status.idle": "2025-01-10T20:31:30.980817Z",
     "shell.execute_reply": "2025-01-10T20:31:30.980526Z",
     "shell.execute_reply.started": "2025-01-10T20:31:30.685211Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS iceberg_raw\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38b7bed-d412-4111-bd7f-bacc1853e412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:31:34.101797Z",
     "iopub.status.busy": "2025-01-10T20:31:34.101442Z",
     "iopub.status.idle": "2025-01-10T20:31:35.948504Z",
     "shell.execute_reply": "2025-01-10T20:31:35.948083Z",
     "shell.execute_reply.started": "2025-01-10T20:31:34.101784Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 17:31:35 WARN HadoopTableOperations: Error reading version hint file file:/home/baptvit/Documents/github/lakehouse-labs/iceberg/warehouse/iceberg_raw/accounts/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/home/baptvit/Documents/github/lakehouse-labs/iceberg/warehouse/iceberg_raw/accounts/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:317)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:103)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:368)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/01/10 17:31:35 WARN HadoopTableOperations: Error reading version hint file file:/home/baptvit/Documents/github/lakehouse-labs/iceberg/warehouse/iceberg_raw/accounts/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File file:/home/baptvit/Documents/github/lakehouse-labs/iceberg/warehouse/iceberg_raw/accounts/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:317)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:103)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:83)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:377)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df.writeTo(\"iceberg_raw.accounts\")\\\n",
    "    .tableProperty(\"changelog.enabled\", \"true\")\\\n",
    "    .tableProperty(\"overwriteSchema\", \"true\")\\\n",
    "    .partitionedBy(\"year\", \"month\")\\\n",
    "    .createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c9eb7f0-1e59-4e55-ab22-cf7d475c0792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T20:33:02.103351Z",
     "iopub.status.busy": "2025-01-10T20:33:02.103130Z",
     "iopub.status.idle": "2025-01-10T20:33:02.932480Z",
     "shell.execute_reply": "2025-01-10T20:33:02.932049Z",
     "shell.execute_reply.started": "2025-01-10T20:33:02.103339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------------+--------------------+--------------------+----------------+---------+-----------+----+-----+---+\n",
      "|country_code|created_at|               email|                iban|                  id|            name| passport|      swift|year|month|day|\n",
      "+------------+----------+--------------------+--------------------+--------------------+----------------+---------+-----------+----+-----+---+\n",
      "|          AR|2024-12-05|brightthomas@exam...|GB56NVYS608859440...|189b84f0-9527-45e...|  Brittany Heath|H58091059|BEBQGBVOSLL|2024|   12|  5|\n",
      "|          AR|2024-12-12|donaldpierce@exam...|GB55LFTZ500270831...|b7e33adb-9bfe-465...|        Ann Cruz|T22953641|HMAQGBCSXE8|2024|   12| 12|\n",
      "|          CA|2024-12-05|harrisondeanna@ex...|GB96EZYO163067760...|bef2df38-4a7a-4b7...| Destiny Jimenez|F75210547|WSHOGBQ55I9|2024|   12|  5|\n",
      "|          GE|2024-12-06|derrick15@example...|GB14AYNQ551881503...|0daad7bc-25b6-446...|Cassidy Jones MD|595954695|VTHYGBZMNOI|2024|   12|  6|\n",
      "|          KR|2024-10-17|powelljason@examp...|GB77AKMZ560580635...|5a424412-b127-4f8...|     Cody Taylor|895549199|INSEGB5PR6S|2024|   10| 17|\n",
      "+------------+----------+--------------------+--------------------+--------------------+----------------+---------+-----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(\"iceberg_raw.accounts\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d837c7f-aadb-41ff-9e8a-47f536929abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cefc41-4945-4aae-b617-4f62f03a74f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094929a-4628-4a50-ab71-1008a7a47d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640febe9-a098-4ff9-a15c-d1a9bf63c1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaa27c-d35d-4c2b-a5fc-d8871b3e06f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67d90d-cb45-4ad3-9d21-e2d0d5d222d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0a276bc-ad1d-48d2-bfd8-fe9d8ada8fdd",
   "metadata": {},
   "source": [
    "### Upsert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f18f8e-f096-44c6-9212-d871cfbf44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [\n",
    "    # Existing entries\n",
    "    dataset[2], \n",
    "    dataset[4], \n",
    "    dataset[7],\n",
    "    dataset[11],\n",
    "    # New entries\n",
    "    *generate_dataset(4, seed=1037)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc774e-2ded-4a09-a263-c91a5d3aa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries:\n",
    "    username = entry['name'].lower().replace(\" \", \".\")\n",
    "    entry['email'] = f\"{username}@domain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7541ce-1cec-4dac-b371-db3d11daeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df = spark.createDataFrame(entries)\\\n",
    "        .withColumn(\"year\", year(col(\"created_at\")))\\\n",
    "        .withColumn(\"month\", month(col(\"created_at\")))\\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"created_at\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe4ed2-dd0f-4704-9658-dd6fcb23f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df.show(8, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1480a-c08f-40ae-8e59-75d9fdc4f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df.createOrReplaceTempView(\"upsert_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af465e53-845a-49e0-ba46-72f24a937791",
   "metadata": {},
   "source": [
    "### Upsert Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75dc232-22af-4601-86e4-353ea04363d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO iceberg_raw.accounts AS target\n",
    "    USING upsert_data AS source ON \n",
    "        target.id = source.id\n",
    "    WHEN MATCHED THEN \n",
    "        UPDATE SET\n",
    "            target.country_code = source.country_code,\n",
    "            target.email = source.email,\n",
    "            target.name = source.name,\n",
    "            target.iban = source.iban,\n",
    "            target.swift = source.swift,\n",
    "            target.passport = source.passport\n",
    "    WHEN NOT MATCHED THEN \n",
    "        INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f2513-a6e0-4f81-9586-88bbaa9e0df8",
   "metadata": {},
   "source": [
    "### Iceberg Metadata (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7c3e2-dbbe-414a-8067-1c0aec8eddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        *\n",
    "    from\n",
    "        iceberg_raw.accounts.history\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa4163-2d82-4f82-b021-c5c460e2f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_changes_df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", \"1492144004614084996\") \\\n",
    "    .load(\"iceberg_raw.accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cf549-f4d6-4644-9d7f-f2f3a257d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_changes_df = spark.read.format(\"iceberg\") \\\n",
    "    .option(\"snapshot-id\", \"6466590935935897141\") \\\n",
    "    .load(\"iceberg_raw.accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606aaee-0f20-467b-833c-6df2f9b04f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
