{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b62605-913b-41a6-8920-4743e6865aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:19:47.728693Z",
     "iopub.status.busy": "2025-01-10T15:19:47.728572Z",
     "iopub.status.idle": "2025-01-10T15:19:47.767282Z",
     "shell.execute_reply": "2025-01-10T15:19:47.766941Z",
     "shell.execute_reply.started": "2025-01-10T15:19:47.728679Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d07b1d-2be6-4b5d-bbbb-00270bba8367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:19:47.767715Z",
     "iopub.status.busy": "2025-01-10T15:19:47.767617Z",
     "iopub.status.idle": "2025-01-10T15:19:47.797090Z",
     "shell.execute_reply": "2025-01-10T15:19:47.796770Z",
     "shell.execute_reply.started": "2025-01-10T15:19:47.767705Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40940756-17f4-44f7-ade4-8cc6a448538a",
   "metadata": {},
   "source": [
    "### Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d262d6-90c2-40f7-b4db-5ed3ec455956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:21:40.695580Z",
     "iopub.status.busy": "2025-01-10T15:21:40.695310Z",
     "iopub.status.idle": "2025-01-10T15:21:40.698913Z",
     "shell.execute_reply": "2025-01-10T15:21:40.698319Z",
     "shell.execute_reply.started": "2025-01-10T15:21:40.695552Z"
    }
   },
   "outputs": [],
   "source": [
    "spark_jar_packages = \",\".join([\n",
    "    \"org.apache.hudi:hudi-spark3.5-bundle_2.12:1.0.0\",\n",
    "    \"org.apache.spark:spark-hive_2.12:3.5.3\",\n",
    "    \"com.google.code.findbugs:jsr305:3.0.2\",\n",
    "    #\"org.apache.thrift:libthrift:0.14.0\",\n",
    "    # \"org.slf4j:slf4j-api:1.7.36\",\n",
    "    # \"org.apache.logging.log4j:log4j-slf4j-impl:2.24.3\",\n",
    "    #\"org.apache.hive:hive-metastore:2.3.9\",\n",
    "    #\"org.apache.hive:hive-exec:2.3.9\",\n",
    "    # \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    # \"com.amazonaws:aws-java-sdk-bundle:1.12.262\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1ab065-8956-42b8-94a0-78f5a38c57fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d417510-85ba-4981-a1c1-4b949381e70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:21:42.023283Z",
     "iopub.status.busy": "2025-01-10T15:21:42.023017Z",
     "iopub.status.idle": "2025-01-10T15:21:46.286933Z",
     "shell.execute_reply": "2025-01-10T15:21:46.286439Z",
     "shell.execute_reply.started": "2025-01-10T15:21:42.023265Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/10 12:21:42 WARN Utils: Your hostname, baptvit resolves to a loopback address: 127.0.1.1; using 192.168.2.129 instead (on interface wlp4s0)\n",
      "25/01/10 12:21:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/baptvit/.ivy2/cache\n",
      "The jars for the packages stored in: /home/baptvit/.ivy2/jars\n",
      "org.apache.hudi#hudi-spark3.5-bundle_2.12 added as a dependency\n",
      "org.apache.spark#spark-hive_2.12 added as a dependency\n",
      "com.google.code.findbugs#jsr305 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e36ae98c-2d53-471f-b04d-8fc35203c0a6;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.hudi#hudi-spark3.5-bundle_2.12;1.0.0 in central\n",
      "\tfound org.apache.hive#hive-storage-api;2.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.apache.spark#spark-hive_2.12;3.5.3 in central\n",
      "\tfound org.apache.hive#hive-common;2.3.9 in central\n",
      "\tfound commons-cli#commons-cli;1.5.0 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound jline#jline;2.14.6 in central\n",
      "\tfound joda-time#joda-time;2.12.5 in central\n",
      "\tfound org.apache.commons#commons-compress;1.23.0 in central\n",
      "\tfound com.tdunning#json;1.8 in local-m2-cache\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.2.19 in central\n",
      "\tfound io.dropwizard.metrics#metrics-jvm;4.2.19 in central\n",
      "\tfound io.dropwizard.metrics#metrics-json;4.2.19 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.15.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.15.2 in central\n",
      "\tfound com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in local-m2-cache\n",
      "\tfound org.apache.hive#hive-exec;2.3.9 in central\n",
      "\tfound commons-io#commons-io;2.16.1 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in local-m2-cache\n",
      "\tfound org.antlr#ST4;4.0.4 in local-m2-cache\n",
      "\tfound org.apache.ivy#ivy;2.5.1 in central\n",
      "\tfound org.datanucleus#datanucleus-core;4.1.17 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound stax#stax-api;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.hive#hive-metastore;2.3.9 in central\n",
      "\tfound javolution#javolution;5.5.1 in local-m2-cache\n",
      "\tfound com.google.protobuf#protobuf-java;3.23.4 in central\n",
      "\tfound com.jolbox#bonecp;0.8.0.RELEASE in central\n",
      "\tfound com.zaxxer#HikariCP;2.5.1 in central\n",
      "\tfound org.apache.derby#derby;10.14.2.0 in central\n",
      "\tfound org.datanucleus#datanucleus-api-jdo;4.2.4 in central\n",
      "\tfound org.datanucleus#datanucleus-rdbms;4.1.19 in central\n",
      "\tfound commons-pool#commons-pool;1.5.4 in local-m2-cache\n",
      "\tfound commons-dbcp#commons-dbcp;1.4 in local-m2-cache\n",
      "\tfound javax.jdo#jdo-api;3.0.1 in central\n",
      "\tfound javax.transaction#jta;1.1 in central\n",
      "\tfound org.datanucleus#javax.jdo;3.2.0-m3 in central\n",
      "\tfound javax.transaction#transaction-api;1.1 in central\n",
      "\tfound org.apache.hive#hive-serde;2.3.9 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in local-m2-cache\n",
      "\tfound org.apache.hive#hive-shims;2.3.9 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-common;2.3.9 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-0.23;2.3.9 in central\n",
      "\tfound org.apache.hive.shims#hive-shims-scheduler;2.3.9 in central\n",
      "\tfound org.apache.hive#hive-llap-common;2.3.9 in central\n",
      "\tfound org.apache.hive#hive-llap-client;2.3.9 in central\n",
      "\tfound org.apache.avro#avro;1.11.2 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.avro#avro-mapred;1.11.2 in central\n",
      "\tfound org.apache.avro#avro-ipc;1.11.2 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.tukaani#xz;1.9 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.16.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.jodd#jodd-core;3.5.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.apache.thrift#libthrift;0.12.0 in central\n",
      "\tfound org.apache.thrift#libfb303;0.9.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      ":: resolution report :: resolve 619ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.15.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.15.2 from central in [default]\n",
      "\tcom.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.23.4 from central in [default]\n",
      "\tcom.jolbox#bonecp;0.8.0.RELEASE from central in [default]\n",
      "\tcom.tdunning#json;1.8 from local-m2-cache in [default]\n",
      "\tcom.zaxxer#HikariCP;2.5.1 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.5.0 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.16.1 from central in [default]\n",
      "\tcommons-dbcp#commons-dbcp;1.4 from local-m2-cache in [default]\n",
      "\tcommons-io#commons-io;2.16.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-pool#commons-pool;1.5.4 from local-m2-cache in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.2.19 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-json;4.2.19 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-jvm;4.2.19 from central in [default]\n",
      "\tjavax.jdo#jdo-api;3.0.1 from central in [default]\n",
      "\tjavax.transaction#jta;1.1 from central in [default]\n",
      "\tjavax.transaction#transaction-api;1.1 from central in [default]\n",
      "\tjavolution#javolution;5.5.1 from local-m2-cache in [default]\n",
      "\tjline#jline;2.14.6 from central in [default]\n",
      "\tjoda-time#joda-time;2.12.5 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from local-m2-cache in [default]\n",
      "\torg.antlr#ST4;4.0.4 from local-m2-cache in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from local-m2-cache in [default]\n",
      "\torg.apache.avro#avro;1.11.2 from central in [default]\n",
      "\torg.apache.avro#avro-ipc;1.11.2 from central in [default]\n",
      "\torg.apache.avro#avro-mapred;1.11.2 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.23.0 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.derby#derby;10.14.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hive#hive-common;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-exec;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-llap-client;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-llap-common;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-metastore;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-serde;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-shims;2.3.9 from central in [default]\n",
      "\torg.apache.hive#hive-storage-api;2.8.1 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-0.23;2.3.9 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-common;2.3.9 from central in [default]\n",
      "\torg.apache.hive.shims#hive-shims-scheduler;2.3.9 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.hudi#hudi-spark3.5-bundle_2.12;1.0.0 from central in [default]\n",
      "\torg.apache.ivy#ivy;2.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-hive_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.thrift#libfb303;0.9.3 from local-m2-cache in [default]\n",
      "\torg.apache.thrift#libthrift;0.12.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]\n",
      "\torg.datanucleus#datanucleus-core;4.1.17 from central in [default]\n",
      "\torg.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]\n",
      "\torg.datanucleus#javax.jdo;3.2.0-m3 from central in [default]\n",
      "\torg.jodd#jodd-core;3.5.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.tukaani#xz;1.9 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\tstax#stax-api;1.0.1 from local-m2-cache in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   68  |   0   |   0   |   2   ||   66  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e36ae98c-2d53-471f-b04d-8fc35203c0a6\n",
      "\tconfs: [default]\n",
      "\t24 artifacts copied, 42 already retrieved (169198kB/179ms)\n",
      "25/01/10 12:21:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HudiWithHive\") \\\n",
    "    .master(\"local[*]\")  \\\n",
    "    .config(\"spark.jars.packages\", spark_jar_packages)  \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f880658-ec91-4643-96c6-ef9ca63b1f66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:21:46.287907Z",
     "iopub.status.busy": "2025-01-10T15:21:46.287722Z",
     "iopub.status.idle": "2025-01-10T15:21:46.584131Z",
     "shell.execute_reply": "2025-01-10T15:21:46.583729Z",
     "shell.execute_reply.started": "2025-01-10T15:21:46.287890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.2.129:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HudiWithHive</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7e531c0bee90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1b86e56-0e24-460d-bdce-1a143f5e4db6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:25:11.407571Z",
     "iopub.status.busy": "2025-01-10T15:25:11.407252Z",
     "iopub.status.idle": "2025-01-10T15:25:11.490633Z",
     "shell.execute_reply": "2025-01-10T15:25:11.490330Z",
     "shell.execute_reply.started": "2025-01-10T15:25:11.407553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3009db51-c620-40e7-a8ca-3180032cbc97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:22:23.830226Z",
     "iopub.status.busy": "2025-01-10T15:22:23.829981Z",
     "iopub.status.idle": "2025-01-10T15:22:23.832568Z",
     "shell.execute_reply": "2025-01-10T15:22:23.832317Z",
     "shell.execute_reply.started": "2025-01-10T15:22:23.830211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define table name and base path\n",
    "database_name = \"default\"\n",
    "table_name = \"hudi_cow\"\n",
    "base_path = \"file:///home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table\"\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"row_1\", \"2021/01/01\", 0, \"bob\", \"v_0\", \"toBeDel0\", 0, 1000000),\n",
    "    (\"row_2\", \"2021/01/01\", 0, \"john\", \"v_0\", \"toBeDel0\", 0, 1000000),\n",
    "    (\"row_3\", \"2021/01/02\", 0, \"tom\", \"v_0\", \"toBeDel0\", 0, 1000000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f10a00c4-3670-419d-9dbe-979b6db710b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:22:24.546498Z",
     "iopub.status.busy": "2025-01-10T15:22:24.546227Z",
     "iopub.status.idle": "2025-01-10T15:22:24.578069Z",
     "shell.execute_reply": "2025-01-10T15:22:24.577689Z",
     "shell.execute_reply.started": "2025-01-10T15:22:24.546474Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"rowId\", StringType(), True),\n",
    "    StructField(\"partitionId\", StringType(), True),\n",
    "    StructField(\"preComb\", LongType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"versionId\", StringType(), True),\n",
    "    StructField(\"toBeDeletedStr\", StringType(), True),\n",
    "    StructField(\"intToLong\", IntegerType(), True),\n",
    "    StructField(\"longToInt\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b966f851-14a6-4f06-bfe1-80fa983b2806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:22:26.663528Z",
     "iopub.status.busy": "2025-01-10T15:22:26.663259Z",
     "iopub.status.idle": "2025-01-10T15:22:26.944558Z",
     "shell.execute_reply": "2025-01-10T15:22:26.944061Z",
     "shell.execute_reply.started": "2025-01-10T15:22:26.663508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+----+---------+--------------+---------+---------+\n",
      "|rowId|partitionId|preComb|name|versionId|toBeDeletedStr|intToLong|longToInt|\n",
      "+-----+-----------+-------+----+---------+--------------+---------+---------+\n",
      "|row_1| 2021/01/01|      0| bob|      v_0|      toBeDel0|        0|  1000000|\n",
      "|row_2| 2021/01/01|      0|john|      v_0|      toBeDel0|        0|  1000000|\n",
      "|row_3| 2021/01/02|      0| tom|      v_0|      toBeDel0|        0|  1000000|\n",
      "+-----+-----------+-------+----+---------+--------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "806151ae-49e0-40d0-b23d-ee9924d67ecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T15:24:33.569050Z",
     "iopub.status.busy": "2025-01-10T15:24:33.568778Z",
     "iopub.status.idle": "2025-01-10T15:24:35.562611Z",
     "shell.execute_reply": "2025-01-10T15:24:35.561793Z",
     "shell.execute_reply.started": "2025-01-10T15:24:33.569037Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o154.save.\n: org.apache.hudi.exception.HoodieMetaSyncException: Could not sync using the meta sync class org.apache.hudi.hive.HiveSyncTool\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:106)\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:72)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:925)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.metaSync(HoodieSparkSqlWriter.scala:923)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.commitAndPerformPostOperations(HoodieSparkSqlWriter.scala:1022)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.writeInternal(HoodieSparkSqlWriter.scala:534)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.$anonfun$write$1(HoodieSparkSqlWriter.scala:190)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.adapter.BaseSpark3Adapter.sqlExecutionWithNewExecutionId(BaseSpark3Adapter.scala:105)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.write(HoodieSparkSqlWriter.scala:212)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:127)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:170)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.hudi.exception.HoodieException: Got runtime exception when hive syncing hudi_cow\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:180)\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:104)\n\t... 57 more\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: failed to sync the table hudi_cow\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:272)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:189)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:177)\n\t... 58 more\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`hudi_cow`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `rowId` string, `preComb` bigint, `name` string, `versionId` string, `toBeDeletedStr` string, `intToLong` int, `longToInt` bigint) PARTITIONED BY (`partitionId` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='file:/home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'file:/home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table' TBLPROPERTIES('spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.schema.part.0'='{\"type\":\"struct\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_commit_seqno\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_record_key\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_partition_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"rowId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"preComb\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"versionId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"toBeDeletedStr\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"intToLong\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"longToInt\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}','spark.sql.sources.schema.partCol.0'='partitionId','spark.sql.sources.schema.numParts'='1','spark.sql.sources.provider'='hudi','spark.sql.create.version'='3.5.3')\n\tat org.apache.hudi.hive.ddl.JDBCExecutor.runSQL(JDBCExecutor.java:70)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:93)\n\tat org.apache.hudi.hive.HoodieHiveSyncClient.createTable(HoodieHiveSyncClient.java:284)\n\tat org.apache.hudi.hive.HiveSyncTool.syncFirstTime(HiveSyncTool.java:398)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:256)\n\t... 60 more\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:267)\n\tat org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:253)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:313)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)\n\tat org.apache.hudi.hive.ddl.JDBCExecutor.runSQL(JDBCExecutor.java:68)\n\t... 64 more\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:199)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.parse.SemanticException: Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:260)\n\tat org.apache.hadoop.hive.ql.parse.StorageFormat.fillStorageFormat(StorageFormat.java:57)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:13004)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11974)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12129)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:330)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:659)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n\t... 15 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hudi.hadoop.HoodieParquetInputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:258)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write data as a Hudi table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhudi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.precombine.field\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreComb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.recordkey.field\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrowId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.partitionpath.field\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpartitionId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.database.name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.table.name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.table.type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCOPY_ON_WRITE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.operation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupsert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.write.hive_style_partitioning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.hive_sync.enable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.hive_sync.mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.hive_sync.jdbcurl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:hive2://localhost:10000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.hive_sync.username\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhoodie.datasource.hive_sync.password\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrangerR0cks!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/github/lakehouse-labs/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o154.save.\n: org.apache.hudi.exception.HoodieMetaSyncException: Could not sync using the meta sync class org.apache.hudi.hive.HiveSyncTool\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:106)\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:72)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:925)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.metaSync(HoodieSparkSqlWriter.scala:923)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.commitAndPerformPostOperations(HoodieSparkSqlWriter.scala:1022)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.writeInternal(HoodieSparkSqlWriter.scala:534)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.$anonfun$write$1(HoodieSparkSqlWriter.scala:190)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.adapter.BaseSpark3Adapter.sqlExecutionWithNewExecutionId(BaseSpark3Adapter.scala:105)\n\tat org.apache.hudi.HoodieSparkSqlWriterInternal.write(HoodieSparkSqlWriter.scala:212)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:127)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:170)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.hudi.exception.HoodieException: Got runtime exception when hive syncing hudi_cow\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:180)\n\tat org.apache.hudi.sync.common.util.SyncUtilHelpers.runHoodieMetaSync(SyncUtilHelpers.java:104)\n\t... 57 more\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: failed to sync the table hudi_cow\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:272)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:189)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:177)\n\t... 58 more\nCaused by: org.apache.hudi.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE IF NOT EXISTS `default`.`hudi_cow`( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `rowId` string, `preComb` bigint, `name` string, `versionId` string, `toBeDeletedStr` string, `intToLong` int, `longToInt` bigint) PARTITIONED BY (`partitionId` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' WITH SERDEPROPERTIES ('hoodie.query.as.ro.table'='false','path'='file:/home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table') STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'file:/home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table' TBLPROPERTIES('spark.sql.sources.schema.numPartCols'='1','spark.sql.sources.schema.part.0'='{\"type\":\"struct\",\"fields\":[{\"name\":\"_hoodie_commit_time\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_commit_seqno\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_record_key\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_partition_path\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"_hoodie_file_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"rowId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"preComb\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"versionId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"toBeDeletedStr\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"intToLong\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"longToInt\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}','spark.sql.sources.schema.partCol.0'='partitionId','spark.sql.sources.schema.numParts'='1','spark.sql.sources.provider'='hudi','spark.sql.create.version'='3.5.3')\n\tat org.apache.hudi.hive.ddl.JDBCExecutor.runSQL(JDBCExecutor.java:70)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:93)\n\tat org.apache.hudi.hive.HoodieHiveSyncClient.createTable(HoodieHiveSyncClient.java:284)\n\tat org.apache.hudi.hive.HiveSyncTool.syncFirstTime(HiveSyncTool.java:398)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:256)\n\t... 60 more\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:267)\n\tat org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:253)\n\tat org.apache.hive.jdbc.HiveStatement.runAsyncOnServer(HiveStatement.java:313)\n\tat org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:253)\n\tat org.apache.hudi.hive.ddl.JDBCExecutor.runSQL(JDBCExecutor.java:68)\n\t... 64 more\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:199)\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:260)\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541)\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:527)\n\tat org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:312)\n\tat org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:562)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)\n\tat org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)\n\tat org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\n\tat org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\n\tat org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\n\tat org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.parse.SemanticException: Cannot find class 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:260)\n\tat org.apache.hadoop.hive.ql.parse.StorageFormat.fillStorageFormat(StorageFormat.java:57)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:13004)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11974)\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12129)\n\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:330)\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:659)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)\n\tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:197)\n\t... 15 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hudi.hadoop.HoodieParquetInputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:258)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "# Write data as a Hudi table\n",
    "df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"preComb\") \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"rowId\") \\\n",
    "    .option(\"hoodie.datasource.write.partitionpath.field\", \"partitionId\") \\\n",
    "    .option(\"hoodie.database.name\", database_name) \\\n",
    "    .option(\"hoodie.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.table.type\", \"COPY_ON_WRITE\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.mode\", \"jdbc\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.jdbcurl\", \"jdbc:hive2://localhost:10000\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.username\", \"hive\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.password\", \"rangerR0cks!\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed3ea5-9426-4bae-9df7-85266c63a3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e1d82-580d-4e9a-a00f-154c1ba0ba8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419299d9-8325-4a6e-83a8-cd8b8ad69e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd6335-b6d6-44c8-ad4d-4273910cc698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1afdd5-de01-4ce2-8429-19d340498f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3f081-7060-4f7d-95a3-8668bf1766ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135aeca-c2ae-444c-b0fc-75cd4759f383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a4136-81cf-41fe-a442-b169b437fc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be4d01-68a2-42ea-b874-f79771e1cfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d89639-9261-43c9-9462-6fa3faa9fb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba8f88-a00e-4b47-b920-1c306bce0a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cd0bc-3854-46ee-9ccf-daa781f39488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bb968-f5a3-4a00-968c-37c359787b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7beffcc-9d51-4125-866a-617b37145c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Load a dataset\n",
    "df = spark.read.csv(\"industry.csv\", header=True, inferSchema=True)\n",
    "df_with_ts = df.withColumn(\"ts\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df8bff-24b4-4879-9c75-c308ca6ce881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03061ab0-558d-4e42-b378-96f5afd29735",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"hudi_table\"\n",
    "database_name = \"default\"\n",
    "base_path = \"file:///home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b4c4c-68a2-4647-827d-fb76eae6c65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write data as a Hudi table\n",
    "df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"Industry\") \\\n",
    "    .option(\"hoodie.datasource.write.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"Industry\") \\\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "    .option(\"hoodie.database.name\", database_name) \\\n",
    "    .option(\"hoodie.datasource.hive_sync.database\", database_name) \\\n",
    "    .option(\"hoodie.datasource.hive_sync.table\", table_name) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82649c-28c1-4fb0-b9e0-8b96bb9cca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data as a Hudi table\n",
    "df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"Industry\") \\\n",
    "    .option(\"hoodie.datasource.write.table.name\", table_name) \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"Industry\") \\\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.partition_fields\", \"ts\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.enable\", \"true\") \\\n",
    "    .option(\"hoodie.database.name\", database_name) \\\n",
    "    .option(\"hoodie.datasource.hive_sync.database\", database_name) \\\n",
    "    .option(\"hoodie.datasource.hive_sync.table\", table_name) \\\n",
    "    .option(\"hoodie.datasource.hive_sync.jdbcurl\", \"jdbc:hive2://ranger-hive:10000/default\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.username\", \"hive\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.password\", \"rangerR0cks!\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c8224-1eef-46d4-b694-2e04d6be5167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06daf81-439d-4c76-8ee2-35332002a4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4d4f5-e0e0-493d-b36a-e534a30899ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595a7ba-96e9-49e6-b06d-3dda52cf1de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255424a7-8443-4863-929c-2570be4394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_ts.write.format(\"hudi\") \\\n",
    "    .options(**hudi_configs) \\\n",
    "    .mode(\"append\")\\\n",
    "    .save(\"file:///home/baptvit/Documents/github/lakehouse-labs/ranger/path/to/hudi/table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9789e3b-bdeb-42f9-9910-6a5a1d939ee6",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa07f69-02fe-4deb-a14c-59de36b3c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entry(faker: Faker, country_codes: list):\n",
    "    return {\n",
    "        \"id\": faker.unique.uuid4(),\n",
    "        \"name\":  faker.name(),\n",
    "        \"email\": faker.email(),\n",
    "        \"passport\": faker.passport_number(),\n",
    "        \"country_code\": random.choice(country_codes),\n",
    "        \"iban\": faker.iban(),\n",
    "        \"swift\": faker.swift11(),\n",
    "        \"created_at\": faker.past_date(start_date='-90d').strftime('%Y-%m-%d')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c245a4-9ace-4bf1-9897-4a8d186ac5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num: int, seed: int):\n",
    "    country_codes = ['US', 'CA', 'JP', 'KR', 'FR', 'GE', 'UK', 'BR', 'AR']\n",
    "    Faker.seed(seed)\n",
    "    faker = Faker()\n",
    "    return [generate_entry(faker, country_codes) for _ in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e5cd7-d544-4b4e-9b89-09b483fef953",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generate_dataset(num=100_000, seed=739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e985a4-3ccc-4b2b-a30f-ee40107bc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(dataset)\\\n",
    "        .withColumn(\"year\", year(col(\"created_at\")))\\\n",
    "        .withColumn(\"month\", month(col(\"created_at\")))\\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"created_at\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb1ff2-6a95-4e4f-8cce-443d04779f9f",
   "metadata": {},
   "source": [
    "### Hudi-Hive Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be022e3c-0ef9-466e-8bcf-08c39ad97783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.database.name\", \"hudi_raw\") \\\n",
    "    .option(\"hoodie.table.name\", \"accounts\") \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"id\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"created_at\") \\\n",
    "    .option(\"hoodie.datasource.write.table.type\", \"COPY_ON_WRITE\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.meta.sync.enable\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.mode\", \"hms\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.partition_fields\", \"year,month\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.partition_extractor_class\", \"org.apache.hudi.hive.MultiPartKeysValueExtractor\") \\\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"s3a://lakehouse-raw/hudi/accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a276bc-ad1d-48d2-bfd8-fe9d8ada8fdd",
   "metadata": {},
   "source": [
    "### Upsert Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f18f8e-f096-44c6-9212-d871cfbf44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = [\n",
    "    # Existing entries\n",
    "    dataset[2], \n",
    "    dataset[4], \n",
    "    dataset[7],\n",
    "    dataset[11],\n",
    "    # New entries\n",
    "    *generate_dataset(4, seed=1037)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc774e-2ded-4a09-a263-c91a5d3aa959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in entries:\n",
    "    username = entry['name'].lower().replace(\" \", \".\")\n",
    "    entry['email'] = f\"{username}@domain.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7541ce-1cec-4dac-b371-db3d11daeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df = spark.createDataFrame(entries)\\\n",
    "        .withColumn(\"year\", year(col(\"created_at\")))\\\n",
    "        .withColumn(\"month\", month(col(\"created_at\")))\\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"created_at\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe4ed2-dd0f-4704-9658-dd6fcb23f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df.show(8, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1480a-c08f-40ae-8e59-75d9fdc4f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df.createOrReplaceTempView(\"upsert_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af465e53-845a-49e0-ba46-72f24a937791",
   "metadata": {},
   "source": [
    "### Upsert Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75dc232-22af-4601-86e4-353ea04363d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_df.write.format(\"hudi\") \\\n",
    "    .option(\"hoodie.database.name\", \"hudi_raw\") \\\n",
    "    .option(\"hoodie.table.name\", \"accounts\") \\\n",
    "    .option(\"hoodie.datasource.write.recordkey.field\", \"id\") \\\n",
    "    .option(\"hoodie.datasource.write.precombine.field\", \"created_at\") \\\n",
    "    .option(\"hoodie.datasource.write.table.type\", \"COPY_ON_WRITE\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.meta.sync.enable\", \"true\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.mode\", \"hms\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.partition_fields\", \"year,month\") \\\n",
    "    .option(\"hoodie.datasource.hive_sync.partition_extractor_class\", \"org.apache.hudi.hive.MultiPartKeysValueExtractor\") \\\n",
    "    .option(\"hoodie.datasource.write.hive_style_partitioning\",\"true\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"s3a://lakehouse-raw/hudi/accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f2513-a6e0-4f81-9586-88bbaa9e0df8",
   "metadata": {},
   "source": [
    "### Hudi Metadata (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7c3e2-dbbe-414a-8067-1c0aec8eddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    call show_commits (\n",
    "        table => 'hudi_raw.accounts',\n",
    "        from_commit => '0'\n",
    "    )    \n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22e223-78d2-4117-a016-751e2f32bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_changes = spark.read.format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"incremental\") \\\n",
    "    .option(\"hoodie.datasource.read.begin.instanttime\", 0) \\\n",
    "    .load(\"s3a://lakehouse-raw/hudi/accounts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087c0a8-4dbf-4414-ab94-226d87c07d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_changes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c606aaee-0f20-467b-833c-6df2f9b04f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
